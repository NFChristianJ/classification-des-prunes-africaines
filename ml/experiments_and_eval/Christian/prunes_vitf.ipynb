{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d7f9d9c-907b-485a-9130-a5ec190b33a4",
   "metadata": {},
   "source": [
    "# Importation des bibliotheques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b5c8a6ab-c069-4964-9b71-2922fbf2a513",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.utils.data import DataLoader\n",
    "from timm import create_model\n",
    "import time \n",
    "import json\n",
    "import splitfolders\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d50e56e-e1c6-4b93-9029-314be1325132",
   "metadata": {},
   "source": [
    "### Ici on va entrainer notre modele sur notre dataset d'entrainement complet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4762f1f-92c3-4a83-8ef9-720e631bbd03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images réorganisées en sous-dossiers par classe.\n"
     ]
    }
   ],
   "source": [
    "# Chemin du dossier train_data\n",
    "dossier_train_data = r\"D:\\african_plums_dataset\\african_plums_Splits\\train\"\n",
    "\n",
    "# Organiser les images en sous-dossiers\n",
    "for file_name in os.listdir(dossier_train_data):\n",
    "    # Utiliser une expression régulière pour extraire la classe\n",
    "    match = re.match(r\"^(.*?)(\\d+)\\.jpeg$\", file_name)\n",
    "    if match:\n",
    "        class_name = match.group(1)\n",
    "        class_dir = os.path.join(dossier_train_data, class_name)\n",
    "        \n",
    "        # Créer le sous-dossier si nécessaire\n",
    "        if not os.path.exists(class_dir):\n",
    "            os.makedirs(class_dir)\n",
    "        \n",
    "        # Déplacer l'image dans le sous-dossier\n",
    "        src_path = os.path.join(dossier_train_data, file_name)\n",
    "        dst_path = os.path.join(class_dir, file_name)\n",
    "        shutil.move(src_path, dst_path)\n",
    "\n",
    "print(\"Images réorganisées en sous-dossiers par classe.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ebd7c92-c49d-4f00-a91c-beb6e4e54259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3603 files belonging to 6 classes.\n",
      "(32, 128, 128, 3)\n",
      "(32,)\n"
     ]
    }
   ],
   "source": [
    "# Les données d'entrainement et de test\n",
    "dossier_train = r\"D:\\african_plums_dataset\\african_plums_Splits\\train\"\n",
    "\n",
    "# Définitions des paramètres pour la taille des images et le nombres d'images traités par lot\n",
    "IMG_SIZE = (128, 128)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Créer un dataset à partir des fichiers d'entraînement\n",
    "train_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    directory=dossier_train,\n",
    "    labels='inferred', # Pour specifier que chaque images de chaque sous repertoires appartient uniquement a cette classe\n",
    "    label_mode='int', # Encodage en entier pour convertir les labels(classes/sous repertoire) en entier afin de faciliter l'entrainement\n",
    "    batch_size=BATCH_SIZE,\n",
    "    image_size=IMG_SIZE,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# Normaliser les images\n",
    "normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
    "\n",
    "# Appliquer la normalisation aux datasets\n",
    "train_dataset = train_dataset.map(lambda x, y: (normalization_layer(x), y))\n",
    "\n",
    "# Afficher la structure des datasets\n",
    "for images, labels in train_dataset.take(1):\n",
    "    print(images.shape)  # Afficher la forme des tenseurs d'images\n",
    "    print(labels.shape)  # Afficher la forme des tenseurs de labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c1a4ff-bc1e-4f78-9138-f366c66b9a43",
   "metadata": {},
   "source": [
    "### Application de la data augmentation\n",
    "\n",
    "Grace a cette technique, lors de l'entrainement du modele sur 10 epoques, le modele aura un equivalent d'environ 30000 images dans sa base d'entrainement. Le principe est le suivant: Lors de l'entrainement du modele sur chaque époques, chaque image recevra les transformations suivante avant d'etre reconnu par le modele comme element d'une classe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c99d9f8a-2b9f-4280-a4f9-a01d47fe822c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3603 images belonging to 6 classes.\n",
      "(32, 128, 128, 3)\n",
      "(32,)\n"
     ]
    }
   ],
   "source": [
    "# Chemin des dossiers train et test\n",
    "dossier_train = r\"D:\\african_plums_dataset\\african_plums_Splits\\train\"\n",
    "\n",
    "# Créer un générateur d'images avec augmentation pour l'entraînement\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=40,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "# Appliquer le générateur aux données d'entraînement\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    directory=dossier_train,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='sparse'  # Utiliser 'sparse' pour les étiquettes entières\n",
    ")\n",
    "\n",
    "# Afficher la structure des datasets\n",
    "for images, labels in train_generator:\n",
    "    print(images.shape)  # Afficher la forme des tenseurs d'images\n",
    "    print(labels.shape)  # Afficher la forme des tenseurs de labels\n",
    "    break  # Pour éviter d'afficher toutes les images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8208ee-2128-47cf-8368-a3ca9b0b9670",
   "metadata": {},
   "source": [
    "### Vérifieons le contenu du répertoire train avant son passage dans le modele et voyons a quoi correspond chaque classe de l'ensemble d'entrainement par rapport aux classes de depart. On va verifier cela avec les indices car plus haut nous avons utiliser un encodage pour encoder nos differentes classes en valeurs entieres (le target encoding) pour faciliter le passage dans le modele\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13efecda-0d39-401e-8e91-cb242a834df6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contenu du répertoire 'train_data' :\n",
      "bruised\n",
      "cracked\n",
      "rotten\n",
      "spotted\n",
      "unaffected\n",
      "unripe\n"
     ]
    }
   ],
   "source": [
    "# Chemin du dossier d'entrainement\n",
    "dossier_train = r\"D:\\african_plums_dataset\\african_plums_Splits\\train\"\n",
    "# Affichage des classes\n",
    "print(\"Contenu du répertoire 'train_data' :\")\n",
    "for file_name in os.listdir(dossier_train):\n",
    "    print(file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "21d6c028-9059-462d-bc59-58519fe9f3d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes et indices correspondants :\n",
      "Indice 0 : bruised\n",
      "Indice 1 : cracked\n",
      "Indice 2 : rotten\n",
      "Indice 3 : spotted\n",
      "Indice 4 : unaffected\n",
      "Indice 5 : unripe\n"
     ]
    }
   ],
   "source": [
    "# Récupérer les noms des classes\n",
    "class_names = train_generator.class_indices\n",
    "class_names = {v: k for k, v in class_names.items()}  # Inverser le dictionnaire pour obtenir les noms des classes\n",
    "\n",
    "# Affichage des noms des classes \n",
    "print(\"Classes et indices correspondants :\")\n",
    "for idx, class_name in class_names.items():\n",
    "    print(f\"Indice {idx} : {class_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2a5278c-7e8d-48c0-b484-2e38bd5b72cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chemin du dataset\n",
    "data_dir = r'D:\\african_plums_dataset\\african_plums_Splits\\train'\n",
    "\n",
    "# Préparer les transformations et le DataLoader\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)), \n",
    "    transforms.ToTensor(), # Ici on normalise les images en les fesant passer de la plage [0,255] en la plage [0,1] car initialement les images sont des tenseurs numpy\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]), # Normalisation appliqué au format des couleurs RGB pour une moyenne de mean et un ecart type de std\n",
    "]) # On aurait pu ajouter d'autre transformation mais ca serait une redondance car cela a deja été fait dans notre section data augmentation.\n",
    "\n",
    "# Application de ce second prétraitement aux données de la base train_data\n",
    "train_dataset = datasets.ImageFolder(root=data_dir, transform=transform)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Charger le modèle ViT pré-entraîné depuis timm avec 6 sorties correspondant au 6 classes\n",
    "model = create_model('vit_base_patch16_224', pretrained=True)\n",
    "model.head = nn.Linear(model.head.in_features, 6)\n",
    "\n",
    "# Définir l'optimiseur et la fonction de perte\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5876a552-7dfc-47a3-931c-7efd6ba71156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.3671, Train Accuracy: 87.79%, Time: 4266.12 seconds\n",
      "Epoch [2/10], Loss: 0.1547, Train Accuracy: 88.79%, Time: 4258.68 seconds\n",
      "Epoch [3/10], Loss: 0.6418, Train Accuracy: 89.20%, Time: 4382.40 seconds\n",
      "Epoch [4/10], Loss: 0.1674, Train Accuracy: 89.65%, Time: 4507.09 seconds\n",
      "Epoch [5/10], Loss: 0.0738, Train Accuracy: 89.73%, Time: 4588.03 seconds\n",
      "Epoch [6/10], Loss: 0.2275, Train Accuracy: 88.98%, Time: 4552.62 seconds\n",
      "Epoch [7/10], Loss: 0.1013, Train Accuracy: 89.56%, Time: 4434.91 seconds\n",
      "Epoch [8/10], Loss: 0.0967, Train Accuracy: 90.23%, Time: 4288.01 seconds\n",
      "Epoch [9/10], Loss: 0.0168, Train Accuracy: 89.18%, Time: 10444.49 seconds\n",
      "Epoch [10/10], Loss: 0.2285, Train Accuracy: 89.79%, Time: 4360.15 seconds\n"
     ]
    }
   ],
   "source": [
    "# Entrainement du model avec affichage de la précision et de la perte\n",
    "for epoch in range(10):  \n",
    "    start_time = time.time() # Début de l'époque\n",
    "    correct_train = 0 # Nombre total d'image correctement predit dans cette epoque\n",
    "    total_train = 0 # Nombre total d'image vu dans cette epoque\n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad() # Reinitialisation des gradients\n",
    "        outputs = model(images) # Predictions\n",
    "        loss = criterion(outputs, labels) # Loss\n",
    "        loss.backward() # Retropropagation\n",
    "        optimizer.step() # Ajustement des parametres\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1) # On reccupere l'indice de la classe predite avec la plus grande probabilité\n",
    "        total_train += labels.size(0) # Mise a jour du nombre d'image vu dans cette epoque jusqu'a cette etapes\n",
    "        correct_train += (predicted == labels).sum().item() # Mise a jour du Nombre total d'image correctement predit dans cette epoque\n",
    "    \n",
    "    train_accuracy = 100 * correct_train / total_train # Calcul de la precision apres chaque etapes\n",
    "    end_time = time.time() # Fin de l'époque \n",
    "    epoch_time = end_time - start_time # Calcul du temps d'exécution \n",
    "    print(f\"Epoch [{epoch+1}/10], Loss: {loss.item():.4f}, Train Accuracy: {train_accuracy:.2f}%, Time: {epoch_time:.2f} seconds\")\n",
    "    #print(f\"Epoch [{epoch+1}/10], Loss: {loss.item():.4f}, Train Accuracy: {train_accuracy:.2f}%\") # affichage de l'epoque, de la perte et de la precision\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548e74a8-058c-4c10-93de-07fd03993f73",
   "metadata": {},
   "source": [
    "## Evaluation du  model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aeddca33-fd8a-4423-8603-0d7302cab25b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chemin du dataset de test\n",
    "test_dir = r'D:\\african_plums_dataset\\african_plums_Splits\\test'\n",
    "\n",
    "# Utiliser les mêmes transformations que pour l'entraînement\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Charger le dataset de test\n",
    "test_dataset = datasets.ImageFolder(root=test_dir, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)  # shuffle=False pour l'évaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "10c331a4-bd5f-4910-8602-c9e943204265",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.6705, Test Accuracy: 69.91%\n"
     ]
    }
   ],
   "source": [
    "model.eval()  # Mode évaluation\n",
    "correct_test = 0\n",
    "total_test = 0\n",
    "test_loss = 0.0\n",
    "\n",
    "with torch.no_grad():  # Désactive le calcul des gradients\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "        \n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total_test += labels.size(0)\n",
    "        correct_test += (predicted == labels).sum().item()\n",
    "\n",
    "test_accuracy = 100 * correct_test / total_test\n",
    "test_loss = test_loss / len(test_loader)  # Perte moyenne par batch\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "57658a3a-3f31-45e6-bb74-16950c9dd2cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrice de confusion:\n",
      "[[ 13   5  16  10  12   8]\n",
      " [  3  16   5   4   3   2]\n",
      " [ 17   4 108   5   1   9]\n",
      " [ 11   0   9  75  39  18]\n",
      " [  2   1   3  29 296  14]\n",
      " [  2   0  11  17  12 124]]\n",
      "\n",
      "Rapport de classification:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     bruised       0.27      0.20      0.23        64\n",
      "     cracked       0.62      0.48      0.54        33\n",
      "      rotten       0.71      0.75      0.73       144\n",
      "     spotted       0.54      0.49      0.51       152\n",
      "  unaffected       0.82      0.86      0.84       345\n",
      "      unripe       0.71      0.75      0.73       166\n",
      "\n",
      "    accuracy                           0.70       904\n",
      "   macro avg       0.61      0.59      0.60       904\n",
      "weighted avg       0.69      0.70      0.69       904\n",
      "\n"
     ]
    }
   ],
   "source": [
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Matrice de confusion\n",
    "print(\"Matrice de confusion:\")\n",
    "print(confusion_matrix(all_labels, all_preds))\n",
    "\n",
    "# Rapport de classification (précision, rappel, F1-score)\n",
    "print(\"\\nRapport de classification:\")\n",
    "print(classification_report(all_labels, all_preds, target_names=test_dataset.classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5985ee-6a16-4b18-9ec1-2832caa521bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fcbf3c50-b85a-48eb-beed-f32e2117c584",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"model_full2.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "20f7d4a5-3481-4809-810b-f096f61b204f",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"model2.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22bd97c1-9e1d-480f-9306-dfca7b2c21a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#class_indices = train_generator.class_indices\n",
    "#with open(\"class_indices.json\", \"w\") as f:\n",
    " #   json.dump(class_indices, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc5d53e-b28f-4f2f-a87a-853508b6a57a",
   "metadata": {},
   "source": [
    "## Evaluation du premier model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7796bdc7-65b0-4e73-a54b-1ea22dea00ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "TEST_DIR = r\"D:\\african_plums_dataset\\african_plums_Splits\\test\"  # Chemin vers vos données de test\n",
    "MODEL_PATH = r\"C:\\Users\\Christian\\Desktop\\PrunesCheck\\model1.pth\"  # Chemin vers votre modèle sauvegardé\n",
    "IMG_SIZE = 224\n",
    "BATCH_SIZE = 32\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "55deaecb-df89-4087-b04d-fdc5d93e67c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 1. Préparation des données de test ---\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "96ebc224-1a5e-4794-93e6-8a2708584667",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Christian\\AppData\\Local\\Temp\\ipykernel_7180\\2462356336.py:7: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(MODEL_PATH))  # Chargement des poids\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "VisionTransformer(\n",
       "  (patch_embed): PatchEmbed(\n",
       "    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "    (norm): Identity()\n",
       "  )\n",
       "  (pos_drop): Dropout(p=0.0, inplace=False)\n",
       "  (patch_drop): Identity()\n",
       "  (norm_pre): Identity()\n",
       "  (blocks): Sequential(\n",
       "    (0): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (1): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (2): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (3): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (4): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (5): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (6): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (7): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (8): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (9): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (10): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "    (11): Block(\n",
       "      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (attn): Attention(\n",
       "        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "        (q_norm): Identity()\n",
       "        (k_norm): Identity()\n",
       "        (attn_drop): Dropout(p=0.0, inplace=False)\n",
       "        (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        (proj_drop): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls1): Identity()\n",
       "      (drop_path1): Identity()\n",
       "      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "      (mlp): Mlp(\n",
       "        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "        (act): GELU(approximate='none')\n",
       "        (drop1): Dropout(p=0.0, inplace=False)\n",
       "        (norm): Identity()\n",
       "        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "        (drop2): Dropout(p=0.0, inplace=False)\n",
       "      )\n",
       "      (ls2): Identity()\n",
       "      (drop_path2): Identity()\n",
       "    )\n",
       "  )\n",
       "  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "  (fc_norm): Identity()\n",
       "  (head_drop): Dropout(p=0.0, inplace=False)\n",
       "  (head): Linear(in_features=768, out_features=6, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset = datasets.ImageFolder(TEST_DIR, transform=transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# --- 2. Rechargement du modèle ---\n",
    "model = create_model('vit_base_patch16_224', pretrained=False)  # Même architecture qu'à l'entraînement\n",
    "model.head = nn.Linear(model.head.in_features, 6)  # Adapté à vos 6 classes\n",
    "model.load_state_dict(torch.load(MODEL_PATH))  # Chargement des poids\n",
    "model = model.to(DEVICE)\n",
    "model.eval()  # Mode évaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5cd2d077-f32b-454c-9882-7d721d6900c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Évaluation ---\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "all_probs = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(DEVICE), labels.to(DEVICE)\n",
    "        outputs = model(images)\n",
    "        probs = torch.softmax(outputs, dim=1)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        \n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_probs.extend(probs.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1ec76483-29c9-4bbf-9190-1e726e0cc076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rapport de classification :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     bruised       0.23      0.27      0.25        64\n",
      "     cracked       0.54      0.45      0.49        33\n",
      "      rotten       0.72      0.76      0.74       144\n",
      "     spotted       0.51      0.32      0.39       152\n",
      "  unaffected       0.76      0.89      0.82       345\n",
      "      unripe       0.77      0.72      0.74       166\n",
      "\n",
      "    accuracy                           0.68       904\n",
      "   macro avg       0.59      0.57      0.57       904\n",
      "weighted avg       0.67      0.68      0.67       904\n",
      "\n",
      "\n",
      "Matrice de confusion :\n",
      "[[ 17   6  17   4  12   8]\n",
      " [  3  15   4   3   6   2]\n",
      " [ 20   5 109   3   2   5]\n",
      " [ 20   0  13  48  58  13]\n",
      " [  8   1   1  20 308   7]\n",
      " [  6   1   7  16  17 119]]\n",
      "Précision pour bruised: 26.56%\n",
      "Précision pour cracked: 45.45%\n",
      "Précision pour rotten: 75.69%\n",
      "Précision pour spotted: 31.58%\n",
      "Précision pour unaffected: 89.28%\n",
      "Précision pour unripe: 71.69%\n"
     ]
    }
   ],
   "source": [
    "# --- 4. Métriques de performance ---\n",
    "print(\"Rapport de classification :\")\n",
    "print(classification_report(all_labels, all_preds, target_names=test_dataset.classes))\n",
    "\n",
    "print(\"\\nMatrice de confusion :\")\n",
    "print(confusion_matrix(all_labels, all_preds))\n",
    "\n",
    "# --- 5. (Optionnel) Analyse par classe ---\n",
    "for i, class_name in enumerate(test_dataset.classes):\n",
    "    class_idx = np.where(np.array(all_labels) == i)[0]\n",
    "    class_acc = np.mean(np.array(all_preds)[class_idx] == i)\n",
    "    print(f\"Précision pour {class_name}: {class_acc:.2%}\")o"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
