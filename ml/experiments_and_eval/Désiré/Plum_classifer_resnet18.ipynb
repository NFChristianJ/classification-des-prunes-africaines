{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JCIA hackathon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß© 1. Chargement des donn√©es et du CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T13:09:16.356115Z",
     "iopub.status.busy": "2025-04-09T13:09:16.355764Z",
     "iopub.status.idle": "2025-04-09T13:09:36.775570Z",
     "shell.execute_reply": "2025-04-09T13:09:36.774894Z",
     "shell.execute_reply.started": "2025-04-09T13:09:16.356085Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "\n",
    "# üìÇ Configuration de base\n",
    "data_dir = \"/kaggle/input/african-plums-quality-and-defect-assessment-data/african_plums_dataset/african_plums\"\n",
    "csv_path = \"/kaggle/input/african-plums-quality-and-defect-assessment-data/african_plums_dataset/plums_data.csv\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# üìä Data Augmentation + Normalisation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# üìÅ Chargement du dataset avec labels bas√©s sur les dossiers\n",
    "dataset = datasets.ImageFolder(root=data_dir, transform=transform)\n",
    "\n",
    "# üì§ Split (80% train / 20% validation)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üß† 2. Fine-tuning de ResNet18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T13:09:36.777169Z",
     "iopub.status.busy": "2025-04-09T13:09:36.776738Z",
     "iopub.status.idle": "2025-04-09T13:09:37.943598Z",
     "shell.execute_reply": "2025-04-09T13:09:37.942659Z",
     "shell.execute_reply.started": "2025-04-09T13:09:36.777130Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 44.7M/44.7M [00:00<00:00, 185MB/s]\n"
     ]
    }
   ],
   "source": [
    "model = models.resnet18(pretrained=True)\n",
    "\n",
    "# üîß Modification de la derni√®re couche pour 6 classes\n",
    "num_ftrs = model.fc.in_features\n",
    "model.fc = nn.Linear(num_ftrs, 6)\n",
    "\n",
    "model = model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚öôÔ∏è 3. Entra√Ænement du mod√®le\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T13:09:37.945443Z",
     "iopub.status.busy": "2025-04-09T13:09:37.945161Z",
     "iopub.status.idle": "2025-04-09T13:18:29.905247Z",
     "shell.execute_reply": "2025-04-09T13:18:29.904257Z",
     "shell.execute_reply.started": "2025-04-09T13:09:37.945422Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Loss: 1.0102\n",
      "Epoch 2/10 - Loss: 0.7045\n",
      "Epoch 3/10 - Loss: 0.6174\n",
      "Epoch 4/10 - Loss: 0.5642\n",
      "Epoch 5/10 - Loss: 0.4812\n",
      "Epoch 6/10 - Loss: 0.4559\n",
      "Epoch 7/10 - Loss: 0.4180\n",
      "Epoch 8/10 - Loss: 0.4006\n",
      "Epoch 9/10 - Loss: 0.3594\n",
      "Epoch 10/10 - Loss: 0.3469\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{epochs} - Loss: {running_loss / len(train_loader):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚úÖ 4. √âvaluation sur validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T13:18:29.906951Z",
     "iopub.status.busy": "2025-04-09T13:18:29.906419Z",
     "iopub.status.idle": "2025-04-09T13:18:46.840129Z",
     "shell.execute_reply": "2025-04-09T13:18:46.839411Z",
     "shell.execute_reply.started": "2025-04-09T13:18:29.906909Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 75.94%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in val_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(f\"Validation Accuracy: {100 * correct / total:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üñºÔ∏è 5. Tester avec une image sp√©cifique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-09T13:18:46.841072Z",
     "iopub.status.busy": "2025-04-09T13:18:46.840857Z",
     "iopub.status.idle": "2025-04-09T13:18:47.070736Z",
     "shell.execute_reply": "2025-04-09T13:18:47.069905Z",
     "shell.execute_reply.started": "2025-04-09T13:18:46.841056Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Pr√©dit : cracked\n"
     ]
    }
   ],
   "source": [
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "img_path = os.path.join(data_dir, \"cracked/cracked_plum_10.png\")\n",
    "\n",
    "img = Image.open(img_path).convert(\"RGB\")\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "img_tensor = transform_test(img).unsqueeze(0).to(device)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = model(img_tensor)\n",
    "    _, pred_class = torch.max(output, 1)\n",
    "\n",
    "# üìå Mapping des indices vers noms de classes\n",
    "idx_to_class = {v: k for k, v in dataset.class_to_idx.items()}\n",
    "print(f\"‚úÖ Pr√©dit : {idx_to_class[pred_class.item()]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 5927133,
     "sourceId": 9694239,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
